{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the csv dataset file to pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Know the dimensions of your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating pandas profile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A pandas profile gives us a statistical overview of the data. Important metrics like missing values, quantile distribution of data, skewness, corelation, std_deviation from the profile report gives us a deeper insight into our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_profiling.ProfileReport(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove features with more than 80% missing values\n",
    "##### Eliminate outliers \n",
    "##### Remove highly corelated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to remove features that are not required goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relook at the dimensions of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the skewness of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.skew().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformation if the data is highly skewed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are many ways to deal with skewed data. Here I have demonstrated how to use the Quantile transformation to deal with skewed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.reset_index(drop=True, inplace=True) \n",
    "\n",
    "for i in range (len(data.columns)):\n",
    "    \n",
    "    q1=data.iloc[:,i].quantile(0.25)\n",
    "    q2=data.iloc[:,i].quantile(0.5)\n",
    "    q3=data.iloc[:,i].quantile(0.75)\n",
    "    \n",
    "    if (q3==0):  \n",
    "        data.iloc[:,i] = data.iloc[:,i].mask(data.iloc[:,i] > 0, 3)\n",
    "        print(data.iloc[:,i].value_counts())\n",
    "\n",
    "    elif (q2==0):  \n",
    "        data.iloc[:,i] = np.where(data.iloc[:,i].between(q2,q3, inclusive=False), 2, data.iloc[:,i])\n",
    "        data.iloc[:,i] = data.iloc[:,i].mask(data.iloc[:,i] >= q3, 3)\n",
    "        print(data.iloc[:,i].value_counts())\n",
    "\n",
    "    elif (q1==0):  \n",
    "        data.iloc[:,i] = np.where(data.iloc[:,i].between(q1,q2, inclusive=False), 1, data.iloc[:,i])\n",
    "        data.iloc[:,i] = np.where(data.iloc[:,i].between(q2,q3), 2, data.iloc[:,i])\n",
    "        data.iloc[:,i] = data.iloc[:,i].mask(data.iloc[:,i] > q3, 3)\n",
    "        print(data.iloc[:,i].value_counts())\n",
    "\n",
    "    elif (q1!=0):  \n",
    "        data.iloc[:,i] = data.iloc[:,i].mask(data.iloc[:,i] <= q1 , 0)\n",
    "        data.iloc[:,i] = np.where(data.iloc[:,i].between(q1,q2), 1, data.iloc[:,i])\n",
    "        data.iloc[:,i] = np.where(data.iloc[:,i].between(q2,q3), 2, data.iloc[:,i])\n",
    "        data.iloc[:,i] = data.iloc[:,i].mask(data.iloc[:,i] > q3, 3)\n",
    "        print(data.iloc[:,i].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension reduction using PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K means performance is poor with high number of features. PCA will help us reduce the number of features thus improving the performance of K means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Scale features if the features have different scales\n",
    "X_std = StandardScaler().fit_transform(data) \n",
    "pca = PCA(n_components=5)\n",
    "principalComponents = pca.fit_transform(X_std)\n",
    "# Plot the explained variances\n",
    "features = range(pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_ratio_, color='blue')\n",
    "plt.xlabel('PCA features')\n",
    "plt.ylabel('variance %')\n",
    "plt.xticks(features)\n",
    "PCA_components = pd.DataFrame(principalComponents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow method to determine k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the first n PCA components where the variance drop amongst components is the least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(1, 10)\n",
    "inertias = []\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters=k)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(PCA_components.iloc[:,:1])\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "plt.plot(ks, inertias, '-o', color='black')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means and  accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = PCA_components.iloc[:,:2]\n",
    "range_n_clusters = [2, 3, 4, 5, 6, 7, 8]\n",
    "labels=[]\n",
    "for n_clusters in range_n_clusters:\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "    \n",
    "    labels.append(cluster_labels)\n",
    "    \n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters                                \n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The inertia is :\", clusterer.inertia_ )\n",
    "    print(pd.Series(cluster_labels).value_counts())\n",
    "    # Compute the silhouette scores for each sample\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniBatchKmeans and accuracy metrics (If our dataset is very large we use the Mini Batch version of K means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = PCA_components.iloc[:,:2]\n",
    "range_n_clusters = [2, 3, 4, 5, 6, 7, 8]\n",
    "labels=[]\n",
    "for n_clusters in range_n_clusters:\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = MiniBatchKMeans(n_clusters=n_clusters, random_state=10, batch_size=100)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "    labels.append(cluster_labels)\n",
    "\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The inertia is :\", clusterer.inertia_ )\n",
    "    \n",
    "    print(pd.Series(cluster_labels).value_counts())\n",
    "    # Compute the silhouette scores for each sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving our K means model using Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clusterer, open(\"save.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
